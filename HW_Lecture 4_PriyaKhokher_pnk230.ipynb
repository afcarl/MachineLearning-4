{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (20% credit)\n",
    "\n",
    "Assume you have a data set as below. It contains records of cars with three features: the type of the car (sports(1) or SUV(2)), the color of the car (red(1) or yellow(2)), and the origin of the car (domestic(1) or imported(2)). And the labels for the data are: stolen(1) and not(0). \n",
    "\n",
    "#### Questions:\n",
    "\n",
    "a) Calculate the following sample probabilities:\n",
    "P(Red|Stolen), P(SUV|Stolen), P(Domestic|Stolen), P(Red|Not Stolen) , P(SUV|Not Stolen), and P(Domestic|Not Stolen)\n",
    "\n",
    "b) Suggest a classification for a red, domestic SUV - whether it will be stolen or not - using Naive Bayes classifier. \n",
    "\n",
    "Please perform all the necessary computations \"by hands\" rather than using python code."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Red = 1,Yellow = 2\n",
    "Stolen = 1, Not Stolen = 0\n",
    "Origin Domestic = 1, Imported = 2\n",
    "Type Sports = 1, SUV = 2\n",
    "P(Red|Stolen) = 3/5,\n",
    "P(SUV|Stolen) = 1/5\n",
    "P(Domestic|Stolen) = 2/5\n",
    "P(Red|Not Stolen) = 2/5\n",
    "P(SUV|Not Stolen) = 3/5\n",
    "P(Domestic|Not Stolen) = 3/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#P(Stolen|{Red,SUV,Domestic}) = \n",
    "3/5.0 * 1/5.0 * 2/5.0 * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07200000000000002"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#P(Not Stolen|{Red, SUV,Domestic}) = \n",
    "2/5.0 * 3/5.0 * 3/5.0 * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Classification 1. P(Stolen/Red)  = P(Red/Stolen)*P(Stolen)/P(Red)\n",
    "(3/5.0)*(0.5)*0.5\n",
    "#Classification 1. P(Not Stolen/Red)  = P(Red/Not Stolen)*P(Not Stolen)/P(Red)\n",
    "(2/5.0)*0.5*0.5\n",
    "#Classification 1. P(Stolen/SUV)  = P(SUV/Stolen)*P(Stolen)/P(SUV)\n",
    "(1/5.0)*(0.5)*(0.4)\n",
    "#Classification 1. P(Not Stolen/Red)  = P(SUV/Not Stolen)*P(Not Stolen)/P(SUV)\n",
    "(3/5.0)*(0.5)(0.4)\n",
    "#Classification 1. P(Stolen/Red)  = P(Domestic/Stolen)*P(Stolen)/P(Domestic)\n",
    "(2/5.0)*(0.5)/(0.5)\n",
    "#Classification 1. P(Not Stolen/Red)  = P(Domestic/Not Stolen)*P(Not Stolen)/P(Domestic)\n",
    "(3/5.0)*(0.5)/(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "y=[1,0,1,0,1,0,1,0,0,1]\n",
    "X=[[1,1,1,2,2,2,2,2,1,1],[1,1,1,1,1,2,2,2,2,1],[1,1,1,1,2,2,2,1,2,2]]\n",
    "data=[y]+X\n",
    "data=pd.DataFrame(data).T\n",
    "data.columns=['Stolen?','Color','Type','Origin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stolen?</th>\n",
       "      <th>Color</th>\n",
       "      <th>Type</th>\n",
       "      <th>Origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stolen?  Color  Type  Origin\n",
       "0        1      1     1       1\n",
       "1        0      1     1       1\n",
       "2        1      1     1       1\n",
       "3        0      2     1       1\n",
       "4        1      2     1       2\n",
       "5        0      2     2       2\n",
       "6        1      2     2       2\n",
       "7        0      2     2       1\n",
       "8        0      1     2       2\n",
       "9        1      1     1       2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainNaiveBayesDiscrete(trainData):\n",
    "      #training discrete Naive Bayes Classifier\n",
    "    tY=trainData.loc[:,trainData.columns[0]]\n",
    "    m=max([trainData[j][i] for j in trainData.columns[1:] for i in trainData.index]) #maximal number of classes in each feature of a training set\n",
    "      #create output data structure for the probabilities - same column labels, rows correspond to values of x and there are two arrays like that for different b\n",
    "    dp=[pd.DataFrame(columns=trainData.columns, index=range(1,m+1)), pd.DataFrame(columns=trainData.columns, index=range(1,m+1))]\n",
    "      #split the training data between two labels\n",
    "    ind1=tY==0\n",
    "    ind2=tY==1\n",
    "    #estimate P(y=b)  \n",
    "    dp[0][trainData.columns[0]][1]=1.0*ind1.sum()/len(trainData.index)\n",
    "    dp[1][trainData.columns[0]][1]=1.0*ind2.sum()/len(trainData.index)\n",
    "      #estimate conditional probabilities P(x|y=b)\n",
    "    for j in trainData.columns[1:]:\n",
    "        for i in range(1,m+1):\n",
    "            dp[0].loc[i,j]=1.0*(trainData[j][ind1]==i).sum()/ind1.sum();\n",
    "            \n",
    "            dp[1].loc[i,j]=1.0*(trainData[j][ind2]==i).sum()/ind2.sum();\n",
    "            \n",
    "    return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classifyNaiveBayesDiscrete(classData,dp):\n",
    "    #classifying using trained discrete Naive Bayes Classifier\n",
    "    Y=classData[classData.columns[0]]*0 #initialize the empty array \n",
    "    for i in classData.index: #for al records to classify\n",
    "    #start with the priors\n",
    "        P1=dp[0][classData.columns[0]][1]; \n",
    "        P2=dp[1][classData.columns[0]][1];\n",
    "    #and multiply them by the corresponding conditional probabilities P(x_i|y=b)\n",
    "        for j in classData.columns[1:]:\n",
    "            P1=P1*dp[0][j][classData[j][i]]\n",
    "            P2=P2*dp[1][j][classData[j][i]]\n",
    "        Y[i]=int(P2>P1) #finally for each record decide which P(y|x) is higher and choose the label\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (25% credit)\n",
    "Consider a following Guassian Naive Bayes problem.\n",
    "We use eight factors to predict if people have diabetes or not. The variabls are:\n",
    "\n",
    "y: The label (0 - no diabetes, 1 - diabetes)\n",
    "\n",
    "t_pre: Number of times pregnant\n",
    "\n",
    "glu: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "\n",
    "blood_p: Diastolic blood pressure (mm Hg)\n",
    "\n",
    "triceps: Triceps skin fold thickness (mm)\n",
    "\n",
    "serum: 2-Hour serum insulin (mu U/ml)\n",
    "\n",
    "b_m: Body mass index (weight in kg/(height in m)^2)\n",
    "\n",
    "pedigree_f: Diabetes pedigree function\n",
    "\n",
    "age: Age (years)\n",
    "#### ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#### Questions:\n",
    "\n",
    "a)Train the classifier: use the training data to estimate prior probabilities $P(y=b)$ as well as the parameters (mean and standard deviation) of the sample distributions $P(x_i|y=b)$.\n",
    "\n",
    "b)Perform the classification for the test sample. \n",
    "\n",
    "c)Compare your result to y_test and report the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNaiveBayes(trainData):\n",
    "  #training Gausian Naive Bayes Classifier\n",
    "  tY=trainData.loc[:,trainData.columns[0]]\n",
    "  ind1=tY==0\n",
    "  ind2=tY==1\n",
    "  dp=pd.DataFrame(columns=trainData.columns, index=['mu1','sigma1','mu2','sigma2'])\n",
    "  #estimate priors\n",
    "  dp[trainData.columns[0]]['mu1']=1.0*sum(ind1)/len(trainData.index)\n",
    "  dp[trainData.columns[0]]['mu2']=1.0*sum(ind2)/len(trainData.index)\n",
    "  #estimate sample distribution paramters for p(xi|y=b)\n",
    "  for i in trainData.columns[1:]:\n",
    "    dp.loc['mu1',i]=(trainData[i][ind1]).mean()\n",
    "    dp.loc['sigma1',i]=(trainData[i][ind1]).std()\n",
    "    dp.loc['mu2',i]=(trainData[i][ind2]).mean()\n",
    "    dp.loc['sigma2',i]=(trainData[i][ind2]).std()\n",
    "  return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyNaiveBayes(classData,dp):\n",
    "  #classifying using trained Gausian Naive Bayes Classifier\n",
    "  Y=classData.loc[:,classData.columns[0]]*0\n",
    "  for j in classData.index:\n",
    "    #start from the priors\n",
    "    P1=dp[classData.columns[0]]['mu1'];\n",
    "    P2=dp[classData.columns[0]]['mu2'];\n",
    "    #multiply by conditional probability densities p(xi|y=b)\n",
    "    for i in classData.columns[1:]:\n",
    "        if dp[i]['sigma1']==0: #if sigma can not be defined (sample does not have variance)\n",
    "            P1=P1*stats.norm.pdf(classData[i][j], loc=dp[i]['mu1'],scale=1) #pick up arbitrary sigma if undefined\n",
    "        else:\n",
    "            P1=P1*stats.norm.pdf(classData[i][j], loc=dp[i]['mu1'],scale=dp[i]['sigma1'])\n",
    "        \n",
    "        if dp[i]['sigma2']==0: #if sigma can not be defined (sample does not have variance)\n",
    "            P2=P2*stats.norm.pdf(classData[i][j], loc=dp[i]['mu2'],scale=1) #pick up arbitrary sigma if undefined\n",
    "        else:\n",
    "            P2=P2*stats.norm.pdf(classData[i][j], loc=dp[i]['mu2'],scale=dp[i]['sigma2']) \n",
    "    Y[j]=int(P2>P1)\n",
    " \n",
    "\n",
    "  return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train=pd.read_csv(\"dia_train.csv\",index_col=0) \n",
    "y_train=data_train.iloc[:,1] \n",
    "X_train=data_train.iloc[:,2:] \n",
    "\n",
    "data_test=pd.read_csv(\"dia_test.csv\",index_col=0)\n",
    "y_test=data_test.iloc[:,1]\n",
    "X_test=data_test.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp = trainNaiveBayes(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>t_pre</th>\n",
       "      <th>glu</th>\n",
       "      <th>blood_p</th>\n",
       "      <th>triceps</th>\n",
       "      <th>serum</th>\n",
       "      <th>b_m</th>\n",
       "      <th>pedigree_f</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu1</th>\n",
       "      <td>0.699153</td>\n",
       "      <td>2.69091</td>\n",
       "      <td>111.467</td>\n",
       "      <td>69.2061</td>\n",
       "      <td>27.2</td>\n",
       "      <td>127.006</td>\n",
       "      <td>31.7091</td>\n",
       "      <td>0.468685</td>\n",
       "      <td>28.3939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.61006</td>\n",
       "      <td>24.6919</td>\n",
       "      <td>11.7133</td>\n",
       "      <td>10.4369</td>\n",
       "      <td>91.4861</td>\n",
       "      <td>6.33761</td>\n",
       "      <td>0.29175</td>\n",
       "      <td>8.53736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu2</th>\n",
       "      <td>0.300847</td>\n",
       "      <td>4.07042</td>\n",
       "      <td>144.141</td>\n",
       "      <td>74.5634</td>\n",
       "      <td>33.4789</td>\n",
       "      <td>209.211</td>\n",
       "      <td>35.2239</td>\n",
       "      <td>0.639042</td>\n",
       "      <td>35.7887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.51862</td>\n",
       "      <td>30.6265</td>\n",
       "      <td>13.7993</td>\n",
       "      <td>9.7627</td>\n",
       "      <td>126.921</td>\n",
       "      <td>6.25849</td>\n",
       "      <td>0.439042</td>\n",
       "      <td>10.2635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               y    t_pre      glu  blood_p  triceps    serum      b_m  \\\n",
       "mu1     0.699153  2.69091  111.467  69.2061     27.2  127.006  31.7091   \n",
       "sigma1       NaN  2.61006  24.6919  11.7133  10.4369  91.4861  6.33761   \n",
       "mu2     0.300847  4.07042  144.141  74.5634  33.4789  209.211  35.2239   \n",
       "sigma2       NaN  3.51862  30.6265  13.7993   9.7627  126.921  6.25849   \n",
       "\n",
       "       pedigree_f      age  \n",
       "mu1      0.468685  28.3939  \n",
       "sigma1    0.29175  8.53736  \n",
       "mu2      0.639042  35.7887  \n",
       "sigma2   0.439042  10.2635  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.11392405063292"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C=classifyNaiveBayes(data_test,dp) #classify test set\n",
    "#classification accuracy (over test set)\n",
    "100.0*sum(C==data_test.y)/len(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.27118644067797"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#also report accuracy of the classifier over the training - it is slightly higher, although not that higher\n",
    "Ct=classifyNaiveBayes(data_train,dp)\n",
    "100.0*sum(Ct==data_train.y)/len(Ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (Credit 25%)\n",
    "We have an artificial data set split, while the training set contains both - labeled (Label_train) and unlabeled (Unlabel) data. Column 'y' is the label, and columns '0','1','2' are categorical variables.\n",
    "\n",
    "#### Questions:\n",
    "\n",
    "a) Use the labeled part data_train to predict the labels of X_Label_test, and report the classification accuracy.\n",
    "\n",
    "b) Improve the classification by using the unlabeled data data_Unlabel and the EM algorithm to predict labels of X_Label_test, and report the new accuracy by EM semi-supervised algorithm (use the same convergence criteria as in the lecture notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train=pd.read_csv(\"EM_train.csv\",index_col=0)\n",
    "y_Label_train=data_train.iloc[:,1] \n",
    "X_Label_train=data_train.iloc[:,2:] \n",
    "\n",
    "data_test=pd.read_csv(\"EM_test.csv\",index_col=0)\n",
    "y_Label_test=data_test.iloc[:,1]\n",
    "X_Label_test=data_test.iloc[:,2:]\n",
    "\n",
    "data_Unlabel=pd.read_csv(\"EM_Unlabel.csv\",index_col=0)\n",
    "X_Unlabel=data_Unlabel.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y  0  1  2\n",
       "3    0  4  6  6\n",
       "135  1  2  2  2\n",
       "25   0  2  6  2\n",
       "111  0  6  6  6\n",
       "76   1  1  2  2\n",
       "24   0  3  4  4\n",
       "89   0  6  1  6\n",
       "22   1  1  3  1\n",
       "34   1  5  1  6"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[          y    0    1    2\n",
       " 1  0.555556    0  0.2    0\n",
       " 2       NaN  0.2    0  0.2\n",
       " 3       NaN  0.2    0    0\n",
       " 4       NaN  0.2  0.2  0.2\n",
       " 5       NaN    0    0    0\n",
       " 6       NaN  0.4  0.6  0.6,           y     0     1     2\n",
       " 1  0.444444   0.5  0.25  0.25\n",
       " 2       NaN  0.25   0.5   0.5\n",
       " 3       NaN     0  0.25     0\n",
       " 4       NaN     0     0     0\n",
       " 5       NaN  0.25     0     0\n",
       " 6       NaN     0     0  0.25]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp=trainNaiveBayesDiscrete(data_train)\n",
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData=pd.concat([y_Label_train,X_Label_train],axis=1)\n",
    "len(y_Label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We correctly classified 8.33333333333 percents of the trips based on the labeled data only\n"
     ]
    }
   ],
   "source": [
    "testdata=pd.concat([y_Label_test,X_Label_test],axis=1)\n",
    "C=classifyNaiveBayesDiscrete(testdata,dp)\n",
    "acc=format(100.0*sum(C==y_Label_test)/len(y_Label_test))\n",
    "print \"We correctly classified {0} percents of the trips based on the labeled data only\".format(acc)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#implementation of Expectation-Maximization algorithm for partially labeled data\n",
    "def EM(X_Label,y_Label,X_Unlabel,dp):\n",
    "  t = 0  \n",
    "  haslabels=len(y_Label)>0\n",
    "\n",
    "  while True:\n",
    "    t = t + 1\n",
    "\n",
    "    classData=X_Unlabel\n",
    "    # Now we want to calculate P(y=1|x) and P(y=2|x) for all observations xj. (these are bunch of scalars)\n",
    "    # we need this to calculate new dp. Basically speaking, for every new iteration we need a new dp.\n",
    "\n",
    "    #for y=1 and y=2\n",
    "\n",
    "    p_x_1=[] #unnormalized P(y=1|x)\n",
    "    p_x_2=[] #unnormalized P(y=2|x)\n",
    "    cols=dp[0].columns\n",
    "\n",
    "    for i in classData.index:\n",
    "        P1=dp[0][cols[0]][1];\n",
    "        P2=dp[1][cols[0]][1];\n",
    "        for j in classData.columns:\n",
    "            P1=P1*dp[0][j][classData[j][i]]\n",
    "            P2=P2*dp[1][j][classData[j][i]]\n",
    "        p_x_1.append(P1)\n",
    "        p_x_2.append(P2)\n",
    "\n",
    "    #Rescale p_x_1 and p_x_2:\n",
    "    summ=np.asarray(p_x_1)+np.asarray(p_x_2)\n",
    "    p_x_1_s=np.asarray(p_x_1)/summ\n",
    "    p_x_2_s=np.asarray(p_x_2)/summ\n",
    "    inds_1 = np.where(np.isnan(p_x_1_s))\n",
    "    inds_2 = np.where(np.isnan(p_x_2_s))\n",
    "    p_x_1_s[inds_1]=0.5\n",
    "    p_x_2_s[inds_2]=0.5\n",
    "    #Now let's calculate P(y=1) and P(y=2)\n",
    "    p_1=p_x_1_s.sum()/len(p_x_1_s)\n",
    "    p_2=p_x_2_s.sum()/len(p_x_2_s)\n",
    "\n",
    "\n",
    "    #Now let's calculate the probability distribution of P(xi|y=1) and P(xi|y=2)\n",
    "    \n",
    "    m=max([classData[j][i] for j in classData.columns for i in classData.index]) #maximal number of classes in each feature of a training set\n",
    "\n",
    "    #create output data structure for the probabilities - new iteration\n",
    "    \n",
    "    dp1=[pd.DataFrame(columns=cols, index=range(1,m+1)), pd.DataFrame(columns=cols, index=range(1,m+1))]\n",
    "\n",
    "    #P(y=b)  \n",
    "    dp1[0][cols[0]][1]=p_1\n",
    "    dp1[1][cols[0]][1]=p_2\n",
    "\n",
    "\n",
    "    #estimate conditional probabilities P(x|y=b) -do we add labeled data to fit?\n",
    "\n",
    "    temp=np.concatenate((np.asmatrix(X_Unlabel),np.asarray(pd.DataFrame(p_x_1_s)),np.asarray(pd.DataFrame(p_x_2_s))), axis=1)\n",
    "    temp=pd.DataFrame(temp)\n",
    "    if haslabels:\n",
    "        temp_l=np.concatenate((np.asmatrix(X_Label),np.asmatrix(1*(y_Label==0)).transpose(),np.asmatrix(1*(y_Label==1)).transpose()),axis=1)\n",
    "        temp_l=pd.DataFrame(temp_l)\n",
    "        pd.concat([temp,temp_l])\n",
    "   \n",
    "\n",
    "    for j in range(1,len(dp[0].T)):\n",
    "        for i in range(len(dp[0])):\n",
    "\n",
    "            dp1[0].iloc[i,j]=temp[temp.iloc[:,j-1]==i+1].iloc[:,-2].sum()/temp.iloc[:,-2].sum()\n",
    "            dp1[1].iloc[i,j]=temp[temp.iloc[:,j-1]==i+1].iloc[:,-1].sum()/temp.iloc[:,-1].sum()\n",
    "\n",
    "        ############################################################################################\n",
    "    # Now we use dp to decide whether to continue our iterations\n",
    "    \n",
    "    if (sum(np.sum((dp1[0]-dp[0])**2))+sum(np.sum((dp1[1]-dp[1])**2)))<0.001: #if dp does not change much\n",
    "        break\n",
    "    else: \n",
    "        dp=dp1  #save new dp and perform next iteration\n",
    "\n",
    "        \n",
    "    ###############################################################################################\n",
    "        #Calculate the log-likelihood\n",
    "        \n",
    "        L=0\n",
    "        \n",
    "        for i in classData.index:\n",
    "            P1=dp[0][cols[0]][1];\n",
    "            P2=dp[1][cols[0]][1];\n",
    "            for j in classData.columns:\n",
    "                P1=P1*dp[0][j][classData[j][i]]\n",
    "                P2=P2*dp[1][j][classData[j][i]]\n",
    "            temp=math.log(P1+P2)\n",
    "            L=L+temp\n",
    "        if haslabels:    \n",
    "          for i in X_Label.index:\n",
    "            yi=y_Label[i]\n",
    "            P=dp[yi][cols[0]][1];\n",
    "            for j in X_Label.columns:\n",
    "                P=P*dp[yi][j][X_Label[j][i]]\n",
    "                \n",
    "            L=L+math.log(P)\n",
    "            \n",
    "        print t,L\n",
    "        \n",
    "        print \"Iteration {0}: log maximum liklihood = {1}\".format(t,L)    \n",
    "        \n",
    "        \n",
    "  return dp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -590.452072842\n",
      "Iteration 1: log maximum liklihood = -590.452072842\n",
      "2 -572.346722303\n",
      "Iteration 2: log maximum liklihood = -572.346722303\n",
      "3 -559.190769042\n",
      "Iteration 3: log maximum liklihood = -559.190769042\n",
      "4 -556.633109435\n",
      "Iteration 4: log maximum liklihood = -556.633109435\n",
      "5 -556.067798435\n",
      "Iteration 5: log maximum liklihood = -556.067798435\n",
      "After EM we correctly classified 94.4444444444 percents of the trips\n"
     ]
    }
   ],
   "source": [
    "#perform EM estimation for theta\n",
    "dpEM=EM(data_train.iloc[:,1:],data_train.iloc[:,0],data_Unlabel,dp)\n",
    "#OS test\n",
    "C=classifyNaiveBayesDiscrete(data_test,dpEM) #classify test data with a new theta given by EM\n",
    "acc=100.0*sum(C==data_test.y)/len(C)\n",
    "print \"After EM we correctly classified {0} percents of the trips\".format(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (Credit 30%)\n",
    "For the similar artifitial data uploaded below:\n",
    "\n",
    "#### Question: \n",
    "\n",
    "a) Apply the EM algorithm (no observed labels, random initial choice of $\\theta$) for clustering the data records into two clusters. Report your result (a vector of cluster numbers for each data record). \n",
    "\n",
    "b) Repeat the clustering 10 times with different random choices of $\\theta$ and analyze the stability of the clustering (matching labels accross different clusterings (use the choice of 0 and 1 labels best matching the previous clustering), estimate average label and its standard error for each record)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 nan\n",
      "Iteration 1: log maximum liklihood = nan\n",
      "2 nan\n",
      "Iteration 2: log maximum liklihood = nan\n",
      "3 nan\n",
      "Iteration 3: log maximum liklihood = nan\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(\"EM_Cluster.csv\", index_col = 0)\n",
    "X=data.iloc[:,1:]\n",
    "np.random.seed(2016)\n",
    "dp = trainNaiveBayesDiscrete(X)\n",
    "dp[0].iloc[0,0]=np.random.uniform(0,1)\n",
    "dp[1].iloc[0,0]=dp[1].iloc[0,0]=1-dp[0].iloc[0,0]\n",
    "\n",
    "for j in range(1,len(dp[0].T)):\n",
    "    b=np.random.uniform(0,1,len(dp[0]))\n",
    "    b=np.asarray(b)/float(np.asarray(b).sum())\n",
    "    dp[0].iloc[:,j]=b\n",
    "for j in range(1,len(dp[1].T)):\n",
    "    b=np.random.uniform(0,1,len(dp[1]))\n",
    "    b=np.asarray(b)/float(np.asarray(b).sum())\n",
    "    dp[1].iloc[:,j]=b\n",
    "dpEM=EM([],[],X,dp)\n",
    "C = classifyNaiveBayesDiscrete(X,dpEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "2      1\n",
       "4      1\n",
       "5      1\n",
       "6      1\n",
       "7      1\n",
       "8      1\n",
       "9      1\n",
       "10     1\n",
       "11     0\n",
       "13     1\n",
       "14     1\n",
       "16     1\n",
       "17     1\n",
       "18     1\n",
       "19     1\n",
       "20     1\n",
       "21     1\n",
       "26     0\n",
       "28     1\n",
       "29     0\n",
       "30     1\n",
       "31     1\n",
       "32     1\n",
       "33     1\n",
       "35     0\n",
       "37     1\n",
       "39     1\n",
       "41     1\n",
       "44     1\n",
       "      ..\n",
       "110    1\n",
       "112    1\n",
       "113    1\n",
       "115    1\n",
       "116    0\n",
       "117    0\n",
       "118    1\n",
       "121    1\n",
       "123    1\n",
       "124    0\n",
       "125    1\n",
       "127    1\n",
       "128    1\n",
       "129    1\n",
       "130    1\n",
       "131    1\n",
       "133    1\n",
       "134    1\n",
       "136    1\n",
       "137    1\n",
       "138    1\n",
       "139    1\n",
       "140    1\n",
       "141    1\n",
       "142    1\n",
       "144    0\n",
       "146    0\n",
       "147    0\n",
       "148    1\n",
       "149    1\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 nan\n",
      "Iteration 1: log maximum liklihood = nan\n",
      "2 nan\n",
      "Iteration 2: log maximum liklihood = nan\n",
      "3 nan\n",
      "Iteration 3: log maximum liklihood = nan\n",
      "1 nan\n",
      "Iteration 1: log maximum liklihood = nan\n",
      "2 nan\n",
      "Iteration 2: log maximum liklihood = nan\n",
      "3 nan\n",
      "Iteration 3: log maximum liklihood = nan\n",
      "1 nan\n",
      "Iteration 1: log maximum liklihood = nan\n",
      "2 nan\n",
      "Iteration 2: log maximum liklihood = nan\n",
      "3 nan\n",
      "Iteration 3: log maximum liklihood = nan\n",
      "1 nan\n",
      "Iteration 1: log maximum liklihood = nan\n",
      "2 nan\n",
      "Iteration 2: log maximum liklihood = nan\n",
      "3 nan\n",
      "Iteration 3: log maximum liklihood = nan\n",
      "1 nan\n",
      "Iteration 1: log maximum liklihood = nan\n",
      "2 nan\n",
      "Iteration 2: log maximum liklihood = nan\n",
      "3 nan\n",
      "Iteration 3: log maximum liklihood = nan\n",
      "1 nan\n",
      "Iteration 1: log maximum liklihood = nan\n",
      "2 nan\n",
      "Iteration 2: log maximum liklihood = nan\n",
      "3 nan\n",
      "Iteration 3: log maximum liklihood = nan\n",
      "1 nan\n",
      "Iteration 1: log maximum liklihood = nan\n",
      "2 nan\n",
      "Iteration 2: log maximum liklihood = nan\n",
      "3 nan\n",
      "Iteration 3: log maximum liklihood = nan\n",
      "1 nan\n",
      "Iteration 1: log maximum liklihood = nan\n",
      "1 nan\n",
      "Iteration 1: log maximum liklihood = nan\n",
      "1 nan\n",
      "Iteration 1: log maximum liklihood = nan\n",
      "2 nan\n",
      "Iteration 2: log maximum liklihood = nan\n",
      "3 nan\n",
      "Iteration 3: log maximum liklihood = nan\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2016)\n",
    "d = []\n",
    "C = pd.DataFrame()\n",
    "#initialize theta randomly\n",
    "for k in range(10):\n",
    "    \n",
    "    dp = trainNaiveBayesDiscrete(X)\n",
    "    dp[0].iloc[0,0]=np.random.uniform(0,1)\n",
    "    dp[1].iloc[0,0]=dp[1].iloc[0,0]=1-dp[0].iloc[0,0]\n",
    " \n",
    "    for j in range(1,len(dp[0].T)):\n",
    "        b=np.random.uniform(0,1,len(dp[0]))\n",
    "        b=np.asarray(b)/float(np.asarray(b).sum())\n",
    "        dp[0].iloc[:,j]=b\n",
    "    for j in range(1,len(dp[1].T)):\n",
    "        b=np.random.uniform(0,1,len(dp[1]))\n",
    "        b=np.asarray(b)/float(np.asarray(b).sum())\n",
    "        dp[1].iloc[:,j]=b\n",
    "    dpEM=EM([],[],X,dp) \n",
    "    #dpee.append(dpEM)\n",
    "    c = classifyNaiveBayesDiscrete(X,dpEM)\n",
    "    d.append(dp)\n",
    "    C[k] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.7\n",
       "2      0.7\n",
       "4      0.6\n",
       "5      0.7\n",
       "6      0.7\n",
       "7      0.7\n",
       "8      0.8\n",
       "9      0.6\n",
       "10     0.6\n",
       "11     0.7\n",
       "13     0.7\n",
       "14     0.7\n",
       "16     0.7\n",
       "17     0.7\n",
       "18     0.7\n",
       "19     0.7\n",
       "20     0.6\n",
       "21     0.6\n",
       "26     0.7\n",
       "28     0.8\n",
       "29     0.7\n",
       "30     0.7\n",
       "31     0.6\n",
       "32     0.7\n",
       "33     0.7\n",
       "35     0.7\n",
       "37     0.6\n",
       "39     0.7\n",
       "41     0.7\n",
       "44     0.7\n",
       "      ... \n",
       "110    0.6\n",
       "112    0.6\n",
       "113    0.8\n",
       "115    0.7\n",
       "116    0.7\n",
       "117    0.7\n",
       "118    0.8\n",
       "121    0.6\n",
       "123    0.7\n",
       "124    0.7\n",
       "125    0.6\n",
       "127    0.6\n",
       "128    0.7\n",
       "129    0.6\n",
       "130    0.7\n",
       "131    0.7\n",
       "133    0.7\n",
       "134    0.8\n",
       "136    0.8\n",
       "137    0.6\n",
       "138    0.7\n",
       "139    0.6\n",
       "140    0.8\n",
       "141    0.7\n",
       "142    0.8\n",
       "144    0.7\n",
       "146    0.7\n",
       "147    0.7\n",
       "148    0.7\n",
       "149    0.6\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.483046\n",
       "2      0.483046\n",
       "4      0.516398\n",
       "5      0.483046\n",
       "6      0.483046\n",
       "7      0.483046\n",
       "8      0.421637\n",
       "9      0.516398\n",
       "10     0.516398\n",
       "11     0.483046\n",
       "13     0.483046\n",
       "14     0.483046\n",
       "16     0.483046\n",
       "17     0.483046\n",
       "18     0.483046\n",
       "19     0.483046\n",
       "20     0.516398\n",
       "21     0.516398\n",
       "26     0.483046\n",
       "28     0.421637\n",
       "29     0.483046\n",
       "30     0.483046\n",
       "31     0.516398\n",
       "32     0.483046\n",
       "33     0.483046\n",
       "35     0.483046\n",
       "37     0.516398\n",
       "39     0.483046\n",
       "41     0.483046\n",
       "44     0.483046\n",
       "         ...   \n",
       "110    0.516398\n",
       "112    0.516398\n",
       "113    0.421637\n",
       "115    0.483046\n",
       "116    0.483046\n",
       "117    0.483046\n",
       "118    0.421637\n",
       "121    0.516398\n",
       "123    0.483046\n",
       "124    0.483046\n",
       "125    0.516398\n",
       "127    0.516398\n",
       "128    0.483046\n",
       "129    0.516398\n",
       "130    0.483046\n",
       "131    0.483046\n",
       "133    0.483046\n",
       "134    0.421637\n",
       "136    0.421637\n",
       "137    0.516398\n",
       "138    0.483046\n",
       "139    0.516398\n",
       "140    0.421637\n",
       "141    0.483046\n",
       "142    0.421637\n",
       "144    0.483046\n",
       "146    0.483046\n",
       "147    0.483046\n",
       "148    0.483046\n",
       "149    0.516398\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.std(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
